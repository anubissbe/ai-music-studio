import soundfile as sf
import librosa
import numpy as np
import torch
from riffusion.riffusion_pipeline import RiffusionPipeline
from riffusion.datatypes import InferenceInput
from PIL import Image
import os
import gc
from pydub import AudioSegment

model = None
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
OUTPUT_FOLDER = '/app/output'
os.makedirs(OUTPUT_FOLDER, exist_ok=True)


def load_model_impl():
    global model
    try:
        if model is not None:
            return True
        print("Loading Riffusion model...")
        model = RiffusionPipeline.load_checkpoint(
            checkpoint="riffusion/riffusion-model-v1",
            device=device,
        )
        print("Model loaded successfully.")
        return True
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        return False


def unload_model_impl():
    global model
    try:
        if model is None:
            return True
        model = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("Model unloaded successfully.")
        return True
    except Exception as e:
        print(f"Error unloading model: {str(e)}")
        return False


def generate_impl(prompt, output_path):
    global model
    try:
        if model is None and not load_model_impl():
            return 0

        inference_input = InferenceInput(
            prompt=prompt,
            alpha=0.5,
            start=None,
            end=None,
            num_inference_steps=50,
            guidance_scale=7.5,
        )

        image = model.riffuse(inference_input, init_image=Image.new('RGB', (512, 512)))
        audio_numpy = torch.tensor(image).cpu().numpy()

        sf.write(output_path, audio_numpy, samplerate=32000)
        convert_to_mp3(output_path, output_path.replace(".wav", ".mp3"))
        duration = librosa.get_duration(y=audio_numpy, sr=32000)
        return duration
    except Exception as e:
        print(f"Error generating music: {str(e)}")
        return 0


def extend_impl(source_track_path, output_path, extend_duration, content_prompt, style_prompt, has_vocals):
    global model
    try:
        if model is None and not load_model_impl():
            return 0

        original_audio, sr = librosa.load(source_track_path, sr=32000)
        combined_prompt = content_prompt
        if style_prompt:
            combined_prompt += f" in the style of {style_prompt}"
        if not has_vocals:
            combined_prompt += ". Instrumental only, no vocals."

        inference_input = InferenceInput(
            prompt=combined_prompt,
            alpha=0.5,
            start=None,
            end=None,
            num_inference_steps=50,
            guidance_scale=7.5,
        )

        image = model.riffuse(inference_input, init_image=Image.new('RGB', (512, 512)))
        extension = torch.tensor(image).cpu().numpy()
        extended_audio = np.concatenate([original_audio, extension])

        sf.write(output_path, extended_audio, samplerate=32000)
        convert_to_mp3(output_path, output_path.replace(".wav", ".mp3"))
        duration = librosa.get_duration(y=extended_audio, sr=32000)
        return duration
    except Exception as e:
        print(f"Error extending track: {str(e)}")
        return 0


def remix_impl(source_track_path, output_path, content_prompt, style_prompt, has_vocals):
    global model
    try:
        if model is None and not load_model_impl():
            return 0

        original_audio, sr = librosa.load(source_track_path, sr=32000, duration=10)
        combined_prompt = f"Remix of: {content_prompt}"
        if style_prompt:
            combined_prompt += f" in the style of {style_prompt}"
        if not has_vocals:
            combined_prompt += ". Instrumental only, no vocals."

        inference_input = InferenceInput(
            prompt=combined_prompt,
            alpha=0.5,
            start=None,
            end=None,
            num_inference_steps=50,
            guidance_scale=7.5,
        )

        image = model.riffuse(inference_input, init_image=Image.new('RGB', (512, 512)))
        remix_audio = torch.tensor(image).cpu().numpy()

        sf.write(output_path, remix_audio, samplerate=32000)
        convert_to_mp3(output_path, output_path.replace(".wav", ".mp3"))
        duration = librosa.get_duration(y=remix_audio, sr=32000)
        return duration
    except Exception as e:
        print(f"Error remixing track: {str(e)}")
        return 0


def convert_to_mp3(wav_path, mp3_path):
    try:
        sound = AudioSegment.from_wav(wav_path)
        sound.export(mp3_path, format="mp3")
        print(f"Converted {wav_path} to {mp3_path}")
    except Exception as e:
        print(f"‚ùå Fout bij conversie naar mp3: {e}")

